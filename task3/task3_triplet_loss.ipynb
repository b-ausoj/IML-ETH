{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d8a1311",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb2efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e6bf47e",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c934f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    The model class, which defines our classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor of the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward pass of the model.\n",
    "\n",
    "        input: x: torch.Tensor, the input to the model\n",
    "\n",
    "        output: x: torch.Tensor, the output of the model\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "200bca95",
   "metadata": {},
   "source": [
    "##  Generate embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3cd3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embedding for each image in the dataset\n",
    "if(os.path.exists('dataset/embeddings.npy') == False):\n",
    "    \"\"\"\n",
    "    Transform, resize and normalize the images and then use a pretrained model to extract \n",
    "    the embeddings.\n",
    "    \"\"\"\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    train_transforms = transforms.Compose([transforms.ToTensor()\n",
    "                                       , transforms.Resize(256) \n",
    "                                       , transforms.CenterCrop(224)\n",
    "                                       , transforms.Normalize([0.6110, 0.5012, 0.3752], [0.2575, 0.2659, 0.2801])\n",
    "                                       ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=50,\n",
    "                                shuffle=False, num_workers=8)\n",
    "    model = resnet50(weights=weights)\n",
    "    model.eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    model.fc = nn.Sequential()\n",
    "\n",
    "    embeddings = []\n",
    "    embedding_size = 2048\n",
    "    num_images = len(train_dataset)\n",
    "    \n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        print(i)\n",
    "        embeddings.append(model(features).T.numpy())\n",
    "\n",
    "    np.save('dataset/embeddings.npy', embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f56cf421",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1f644fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletsDataset(Dataset):\n",
    "    def __init__(self, triplets, file_to_tensor):\n",
    "        super().__init__()\n",
    "        self.triplets = triplets\n",
    "        self.file_to_tensor = file_to_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        line = self.triplets[index].split()\n",
    "        a = self.file_to_tensor[line[0]]\n",
    "        p = self.file_to_tensor[line[1]]\n",
    "        n = self.file_to_tensor[line[2]]\n",
    "        return a, p, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5debdc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, train=True):   \n",
    "    \n",
    "    triplets = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            triplets.append(line)\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=None)\n",
    "    filenames = [s[0].split('/')[-1].replace('.jpg', '').replace('food\\\\', '') for s in train_dataset.samples]\n",
    "\n",
    "    embeddings = np.swapaxes(np.load('dataset/embeddings.npy'), 1, 2)\n",
    "\n",
    "    file_to_tensor = {}\n",
    "    for i in range(200):\n",
    "        for j in range(50):\n",
    "            file_to_tensor[filenames[j+i*50]] = torch.tensor(embeddings[i][j])\n",
    "\n",
    "    return triplets, file_to_tensor\n",
    "\n",
    "TRAIN_TRIPLETS = 'train_triplets.txt'\n",
    "TEST_TRIPLETS = 'test_triplets.txt'\n",
    "\n",
    "# load the training and testing data\n",
    "triplets, file_to_tensor = get_data(TRAIN_TRIPLETS)\n",
    "triplets_test, file_to_tensor_test = get_data(TEST_TRIPLETS, train=False)\n",
    "\n",
    "\n",
    "full_dataset = TripletsDataset(triplets, file_to_tensor)\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "validation_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(full_dataset, [train_size, validation_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = TripletsDataset(triplets_test, file_to_tensor_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa793f98",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ec8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model = Net()\n",
    "model.to(device)\n",
    "n_epochs = 5\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) \n",
    "criterion = nn.TripletMarginWithDistanceLoss(distance_function=nn.CosineSimilarity())\n",
    "dist = nn.CosineSimilarity()\n",
    "\n",
    "last_score = 0\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch={epoch}')  \n",
    "    running_loss = 0.0 \n",
    "    model.train()\n",
    "    i = 0    \n",
    "    for a, p, n in train_loader:\n",
    "        a_out = model(a)\n",
    "        p_out = model(p)\n",
    "        n_out = model(n)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(a_out, p_out, n_out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "        if i % 500 == 499:    # print every 500 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 500:.3f}')\n",
    "            running_loss = 0.0\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    n_correct = 0\n",
    "    for a, p, n in validation_loader:\n",
    "        a_out = model(a)\n",
    "        p_out = model(p)\n",
    "        n_out = model(n)\n",
    "\n",
    "        res_cos = dist(a_out, p_out)-dist(a_out, n_out)\n",
    "        res_cos[res_cos >= 0] = 0\n",
    "        res_cos[res_cos < 0] = 1\n",
    "\n",
    "        n_correct += res_cos.sum().item()\n",
    "\n",
    "        loss = criterion(a_out, p_out, n_out)\n",
    "        running_loss += loss.item()\n",
    "        total += len(a_out)\n",
    "    score = n_correct/total\n",
    "    print(score)\n",
    "    if (last_score >= score):\n",
    "        break\n",
    "    else:\n",
    "        last_score = score\n",
    "        torch.save(model.state_dict, './model_saved')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67adf8a2",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f970fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results.txt\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "model.eval()\n",
    "dist = nn.CosineSimilarity()\n",
    "predictions = []\n",
    "# Iterate over the test data\n",
    "k = 0\n",
    "with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "    for a, p, n in test_loader:\n",
    "        predicted_a = model(a)\n",
    "        predicted_p = model(p)\n",
    "        predicted_n = model(n)\n",
    "        \n",
    "        prediction = dist(predicted_a, predicted_p) - dist(predicted_a, predicted_n)\n",
    "        prediction[prediction >= 0] = 1\n",
    "        prediction[prediction < 0] = 0\n",
    "        for x in prediction.numpy():\n",
    "            predictions.append(x)\n",
    "\n",
    "    predictions = np.stack(predictions)\n",
    "    \n",
    "np.savetxt(\"results.txt\", predictions, fmt='%i')\n",
    "print(\"Results saved to results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8b71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6607750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
